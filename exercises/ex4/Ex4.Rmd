---
title: "Exercise 4"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(arrow)
library(ggraph)
library(igraph)
library(wru)
library(gender)
library(stringr)
library(mice)
```

## Load data
```{r load-data}
# change to your own path!
data_path <- "C:/Users/mattl/OneDrive/Documents/Education/Masters/Courses/Spring Semester/Organizational Network Analysis/2022-ona-assignments/exercises/ex4/"
applications <- read_parquet(paste0(data_path,"app_data_sample.parquet"))
edges <- read_csv(paste0(data_path,"edges_sample.csv"))
applications
```

## Get gender for examiners

We'll get gender based on the first name of the examiner, which is recorded in the field `examiner_name_first`. We'll use library `gender` for that, relying on a modified version of their own [example](https://cran.r-project.org/web/packages/gender/vignettes/predicting-gender.html).

Note that there are over 2 million records in the applications table -- that's because there are many records for each examiner, as many as the number of applications that examiner worked on during this time frame. Our first step therefore is to get all *unique* names in a separate list `examiner_names`. We will then guess gender for each one and will join this table back to the original dataset. So, let's get names without repetition:

```{r gender-1}
#install_genderdata_package() # only run this line the first time you use the package, to get data for it

# get a list of first names without repetitions
examiner_names <- applications %>% 
  distinct(examiner_name_first)

#Now let's use function `gender()` as shown in the example for the package to attach a gender and probability to each name and put the results into the table `examiner_names_gender`

# get a table of names and gender
examiner_names_gender <- examiner_names %>% 
  do(results = gender(.$examiner_name_first, method = "ssa")) %>% 
  unnest(cols = c(results), keep_empty = TRUE) %>% 
  select(
    examiner_name_first = name,
    gender,
    proportion_female
  )
examiner_names_gender


# Finally, let's join that table back to our original applications data and discard the temporary tables we have just created to reduce clutter in our environment.


# remove extra columns from the gender table
examiner_names_gender <- examiner_names_gender %>% 
  select(examiner_name_first, gender)
# joining gender back to the dataset
applications <- applications %>% 
  left_join(examiner_names_gender, by = "examiner_name_first")
# cleaning up
rm(examiner_names)
rm(examiner_names_gender)
gc()
```

## Guess the examiner's race

We'll now use package `wru` to estimate likely race of an examiner. Just like with gender, we'll get a list of unique names first, only now we are using surnames.

```{r race-1}
examiner_surnames <- applications %>% 
  select(surname = examiner_name_last) %>% 
  distinct()
examiner_surnames

examiner_race <- predict_race(voter.file = examiner_surnames, surname.only = T) %>% 
  as_tibble()
examiner_race

#As you can see, we get probabilities across five broad US Census categories: white, black, Hispanic, Asian and other. (Some of you may correctly point out that Hispanic is not a race category in the US Census, but these are the limitations of this package.)

#Our final step here is to pick the race category that has the highest probability for each last name and then join the table back to the main applications table. See this example for comparing values across columns: [https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-rowwise/](https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-rowwise/). And this one for `case_when()` function: [https://dplyr.tidyverse.org/reference/case_when.html](https://dplyr.tidyverse.org/reference/case_when.html).

examiner_race <- examiner_race %>% 
  mutate(max_race_p = pmax(pred.asi, pred.bla, pred.his, pred.oth, pred.whi)) %>% 
  mutate(race = case_when(
    max_race_p == pred.asi ~ "Asian",
    max_race_p == pred.bla ~ "black",
    max_race_p == pred.his ~ "Hispanic",
    max_race_p == pred.oth ~ "other",
    max_race_p == pred.whi ~ "white",
    TRUE ~ NA_character_
  ))
examiner_race


# Let's join the data back to the applications table.

# removing extra columns
examiner_race <- examiner_race %>% 
  select(surname,race)
applications <- applications %>% 
  left_join(examiner_race, by = c("examiner_name_last" = "surname"))
rm(examiner_race)
rm(examiner_surnames)
gc()
```

## Examiner's tenure 

To figure out the timespan for which we observe each examiner in the applications data, let's find the first and the last observed date for each examiner. We'll first get examiner IDs and application dates in a separate table, for ease of manipulation. We'll keep examiner ID (the field `examiner_id`), and earliest and latest dates for each application (`filing_date` and `appl_status_date` respectively). We'll use functions in package `lubridate` to work with date and time values.

```{r tenure-1}

examiner_dates <- applications %>% 
  select(examiner_id, filing_date, appl_status_date) 
examiner_dates


# The dates look inconsistent in terms of formatting. Let's make them consistent. We'll create new variables `start_date` and `end_date`.


examiner_dates <- examiner_dates %>% 
  mutate(start_date = ymd(filing_date), end_date = as_date(dmy_hms(appl_status_date)))


# Let's now identify the earliest and the latest date for each examiner and calculate the difference in days, which is their tenure in the organization.


examiner_dates <- examiner_dates %>% 
  group_by(examiner_id) %>% 
  summarise(
    earliest_date = min(start_date, na.rm = TRUE), 
    latest_date = max(end_date, na.rm = TRUE),
    tenure_days = interval(earliest_date, latest_date) %/% days(1)
    ) %>% 
  filter(year(latest_date)<2018)
examiner_dates


# Joining back to the applications data.

applications <- applications %>% 
  left_join(examiner_dates, by = "examiner_id")
rm(examiner_dates)
gc()
```
## Application Processing Time
```{r}
# Since an application can only be issued or abandoned, one or the other will always be NA, therefore I will combine the columns

applications$appl_end_date <- paste(applications$patent_issue_date, applications$abandon_date, sep=',')

# Then I will clean up the column by removing instances of commas and NA's
applications$appl_end_date <- gsub('NA', "", as.character(applications$appl_end_date))
applications$appl_end_date <- gsub(',', "", as.character(applications$appl_end_date))

# Ensure date format is consistent for both columns
applications$appl_end_date <- as.Date(applications$appl_end_date, format="%Y-%m-%d")
applications$filing_date <- as.Date(applications$filing_date, format="%Y-%m-%d")

# Finding the difference in days between the application end date and the filing date
applications$appl_proc_days <- as.numeric(difftime(applications$appl_end_date, applications$filing_date, units=c("days")))

# Remove instances where the filing date happens after the issue or abandon dates (these must be mistakes as this shouldnt be possible
applications <- applications %>% filter(appl_proc_days >=0 | appl_proc_days != NA)
```


## Additional Data Cleaning Prior to Modelling
```{r}
# Find the count of missing values in each column
sapply(applications, function(x) sum(is.na(x)))
```
```{r}
# Remove unnecessary columns for modelling
applications_mod <- subset(applications, select = -c(filing_date, abandon_date, earliest_date, appl_end_date, appl_status_date, patent_issue_date, latest_date, examiner_name_middle, patent_number))

sapply(applications_mod, function(x) sum(is.na(x)))
```
```{r}
applications_mod <- applications_mod %>% drop_na(examiner_id)
```

Use mice to impute missing values
```{r}
applications_mod$gender <- as.factor(applications_mod$gender)

applications_mod_imp <- complete(mice(applications_mod, m=3, maxit=3))
rm(applications_mod)
```
## Generating Network Information

```{r}
# first get work group for each examiner and limit to our two wgs of interest
examiner_aus = distinct(subset(applications_mod_imp, select=c(examiner_art_unit, examiner_id)))

# we eventually want to make a network with nodes colored by work group, so lets add that indicator
examiner_aus$wg = substr(examiner_aus$examiner_art_unit, 1,3)

# restrict down to our selected art units to reduce merging complexity later on
examiner_aus = examiner_aus[examiner_aus$wg==162 | examiner_aus$wg==219,]

# now we will merge in the aus df on applications 
adv_network = merge(x=edges, y=examiner_aus, by.x="ego_examiner_id", by.y="examiner_id", all.x=TRUE)
adv_network = adv_network %>% rename(ego_art_unit=examiner_art_unit, ego_wg=wg)

adv_network = drop_na(adv_network)

# now repeat for the alter examiners
adv_network = merge(x=adv_network, y=examiner_aus, by.x="alter_examiner_id", by.y="examiner_id", all.x=TRUE)
adv_network = adv_network %>% rename(alter_art_unit=examiner_art_unit, alter_wg=wg)
adv_network = drop_na(adv_network)

```

```{r}
# we are left with 870 edges corresponding to instances of examiners in wg173 or wg175 asking for advice
egoNodes = subset(adv_network, select=c(ego_examiner_id,ego_art_unit, ego_wg)) %>% rename(examiner_id=ego_examiner_id,art_unit=ego_art_unit,wg=ego_wg)
alterNodes = subset(adv_network, select=c(alter_examiner_id,alter_art_unit, alter_wg))%>% rename(examiner_id=alter_examiner_id,art_unit=alter_art_unit,wg=alter_wg)
nodes = rbind(egoNodes, alterNodes)
nodes = distinct(nodes)

# problem: when we reduce to the list of distinct nodes, we actually have more than we should, since some examiners move amongst art units/wgs in this data subset
nodes = nodes %>% group_by(examiner_id) %>% summarise(examiner_id=first(examiner_id), art_unit=first(art_unit), wg=first(wg))
```

```{r}
network <- graph_from_data_frame(d=adv_network, vertices=nodes, directed=TRUE)
network

# Calculate the node metrics
Degree <- degree(network)
Closeness <- closeness(network)
Betweenness <- betweenness(network)
Eig <- evcent(network)$vector


comp <- data.frame(nodes, Degree, Eig, Closeness, Betweenness)   
comp 
```
Now, we will merge the centrality measurements back into our imputed dataset
```{r}
applications_final <- merge(x=applications_mod_imp, y=comp, by='examiner_id', all.x=TRUE)

```

```{r}

# Filter to only include selected work groups
applications_final = applications_final %>% filter(wg==162 | wg==219)

# Remove NaN values for instances where examiner didnt ask for any advice
applications_final <- drop_na(applications_final)

```

## Modelling

First we will do a simple model with no interaction variables.
```{r}
lm1 <- lm(appl_proc_days~Eig + Degree + Closeness + Betweenness + gender + tenure_days, data=applications_final)
summary(lm1)
```
 Some notes/insights/hypothesis based on these results: 
 - Stat significant model and most variables are significant, except for any influence by gender
 
 - Baseline processing time is 1604 days (Female, 0 days of tenure, never sought advice)
 
 - Increasing an examiners eigenvector importance from 0 to 1 would be expected to decrease processing time by 236 days (i.e, the more important). This would make sense, if a particular examiner had a lot of influence over an advice network, with many other examiners seeking them for advice, they would likely be a subject matter expert and would need to take less time seeking out information online or from other individuals to get the answers they needed to process the application
 
 - An examiner seeking advice one additional time from another examiner (an increase in degree by 1) results in an increase in processing time of about 3 days. This could make sense, as seeking out additional advice, or having those come seek advice from you, is time consuming and could take time away from processing the applications
 
 - An increase in closeness centrality from 0 to 1 would be expected to correspond to a 138 day decrease in processing time. This could also make sense, as a high closeness centrality would correspond to an examiner being well connected within the network. Even if they don't know someone who is an SME on a topic, they likely know someone who knows someone. This may make the process of finding the information they are looking for easier, and speed up the time taken to process the application
 
 - A 1 unit increase in betweenness centrality corresponds to a 27 day increase in processing time. Similar to degree centrality, if an examiner is a main gate for information to pass in the network, this could be time consuming and take time away from them processing applications themselves.
 
 - Finally, an increase in tenure by 1 day corresponds to a slight decrease in processing time. This seems intuitive that more experience would result in quicker processing times.

 
Adding Interaction Variables
```{r}
lm2 <- lm(appl_proc_days~Eig + Degree + Closeness + Betweenness + gender + tenure_days + Degree*gender + Eig*gender + Closeness*gender + Betweenness*gender, data=applications_final)
summary(lm2)
```
Some notes/insights/hypothesis based on these results: 
 - Stat significant model and all variables are significant with at least 90% confidence
 
 - Relationships for variables from previous model are similar, except closeness now has a more positive relationship with processing time (a unit increase in closeness centrality correlates with a 48 day increase in processing time)
 
 - Tenure still has a reducing effect on processing time 
 
 - Unit increase in degree for male examiners reduces processing time by 4.5 days
 
 - Unit increase in eig importance for male examiners increases processing time by 7270 days
 
 - Unit increase in closeness for males decreases processing time by 191 days
 
 - Unit increase in betweenness for males decreases processing time by 78 days
 
 It should be noted that it is difficult to infer how a change by one or many examiners would impact the processing times since each of the centrality scores are interrelated. 
 

 

